delete all
import all


(delete, import)
    if table exists:
        delete table
    mark_create_table()
    import()

(delete, NO import):
    if table exists:
        delete table

(NO delete, import):
    if table does not exist:
        mark_create_table()
    import()

(NO delete, NO import):
    bye

Import():
    csv_cols = read cols from first CSV

    if needs_create_table:
        load mapcolumns.txt
        apply mapcolumns to csv_cols


env LDFLAGS="-I/usr/local/opt/openssl/include -L/usr/local/opt/openssl/lib" python3 -m pip --no-cache install psycopg2==2.9.9

Alright try this


env LDFLAGS="-I/opt/homebrew/opt/openssl/include -L/opt/homebrew/opt/openssl/lib" python3 -m pip --no-cache install psycopg2==2.9.9

"""
def init_import(needs_create_table: bool):
    folder = Path(g.C_CSV_DIRECTORY)
    files = sorted(folder.glob('*.csv'))

    if len(files) == 0:
        print("No CSV found")
        return

    active_chunk = []
    max_chunk_len = 50

    def send_chunk():
        if len(active_chunk) == 0:
            return

        col_count = len(active_chunk[0])

        placeholder_values = ",".join([f":v{i}" for i in range(col_count)])
        stmt = "INSERT INTO {} VALUES ({})".format(g.C_TABLE_NAME, placeholder_values)
        values = [
            {
                f"v{i}": val
                for i, val in enumerate(row)
            }
            for row in active_chunk
        ]

        with engine.begin() as conn:
            conn.execute(text(stmt), values)

        active_chunk.clear()

    awaiting_columns = True
    count = 0
    tbl_columns = None

    pbar_rows = tqdm(desc="Processing rows")
    pbar_files = tqdm(total=len(files), desc="Files read", colour="#E36576")

    for file in files:
        logging.info("Reading file {}".format(file))
        pbar_rows.set_description("Processing rows from file \"{}\"".format(str(file)))

        # Open the file
        f =  open(file, newline='')

        reader = csv.reader(f)
        for i, row in enumerate(reader):
            pbar_rows.update()
            if i == 0:
                if awaiting_columns:
                    awaiting_columns = False
                    tbl_columns = finalize_columns(row)
                    # if needs_create_table:
                        # create_table(tbl_columns)
                continue

            active_chunk.append(row)
            if len(active_chunk) >= max_chunk_len:
                count += len(active_chunk)
                send_chunk()

        count += len(active_chunk)
        send_chunk()

        f.close()
        pbar_files.update()

    pbar_rows.close()
    pbar_files.close()

    count += len(active_chunk)
    send_chunk()

    logging.info("{} row(s) added".format(count))

"""



Hi

So now in this version of the script, the use of columns.txt file is removed completely. Instead the columns are read directly from the CSV files and any mappings can be applied using mapcolumns.txt.

There is a slight change in the format of mapcolumns.txt. There is a ">" between the column names i.e each mapping is of the form:

Column A > Column B

This is so that spaces do not create any confusion because otherwise e.g "Company Name Company" could mean either "Company > Name Company" or "Company Name > Company". So we add the ">" for clarity.

There is also an additional feature that you can ignore columns from the CSV by mapping them to the character '*'. for example

Phone > *

This will ignore the "Phone" column from the CSV files i.e it will not store it in the database.

Please let me know if you have any feedback or face any problems

Thanks